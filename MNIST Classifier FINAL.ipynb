{"cells":[{"metadata":{},"cell_type":"markdown","source":"Voting Classifier\n\nWe want to train a Random Forest Classifier, Extra Trees Classifier, Linear Support Vector Classifier, and an MLP Classifier.\n\nWe combine the 4 Classifiers in a Voting Classifier since it often achieves a higher accuracy than the best classifier in the group.\n\nIt is a simple way to combine the predictions of each class and predict the class with the most votes."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sklearn\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import accuracy_score\n\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We import our MNIST data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train_val, x_test, y_train_val, y_test = train_test_split(\n    mnist.data, mnist.target, test_size=10000)\n\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_val, y_train_val, test_size=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split our data into training, validation, and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier = RandomForestClassifier(n_estimators=100)\nextra_trees_classifier = ExtraTreesClassifier(n_estimators=100)\nsvm_classifier = LinearSVC()\nmlp_classifier = MLPClassifier()\n\nestimators = [random_forest_classifier, extra_trees_classifier, svm_classifier, mlp_classifier]\nfor estimator in estimators:\n    estimator.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fit the models with the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"[estimator.score(x_val, y_val) for estimator in estimators]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We check the score of our models.\n\nSVC seems a bit low with significantly less than 95%."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_classifiers = [\n    (\"random_forest_classifier\", random_forest_classifier),\n    (\"extra_tress_classifier\", extra_trees_classifier),\n    (\"svm_classifier\", svm_classifier),\n    (\"mlp_classifier\", mlp_classifier),\n]\n\nvoting_classifier = VotingClassifier(grouped_classifiers)\nvoting_classifier.fit(x_train, y_train)\nvoting_classifier.score(x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we group the classifier into a voting classifier and we get 0.9681.\n\nNot bad, better than any of the other independent classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.set_params(svm_classifier=None)\ndel voting_classifier.estimators_[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try getting rid of the SVC to see if our model improves."},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.estimators\nvoting_classifier.estimators_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.voting = \"soft\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use soft voting and predict the class with the highest probability averaged over all the individual classifier.\n\nIt usually does better than hard voting since it gives more weight to the classifiers that are more confident."},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.score(x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get 0.9679 which isn't bad.\n\nLet's try our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get 0.9657 which isn't as good as our validation set but still pretty good."},{"metadata":{"trusted":true},"cell_type":"code","source":"[estimator.score(x_test, y_test) for estimator in voting_classifier.estimators_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Extra Trees Classifier is still better with 0.9723."},{"metadata":{},"cell_type":"markdown","source":"Let's try making a stacking ensemble with a blender to see if our model improves.\n\nWe take the classifiers from earlier to make predictions on the validation set and make a new training set with the predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_validation_predictions = np.empty((len(x_val), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    x_validation_predictions[:, index] = estimator.predict(x_val)\n\nx_validation_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True)\nrandom_forest_blender.fit(x_validation_predictions, y_val)\nrandom_forest_blender.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just trained our blender and created a stacking ensemble.\n\nLet's try it out on our test set and compare it to out voting classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_predictions = np.empty((len(x_test), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    x_test_predictions[:, index] = estimator.predict(x_test)\n    \ny_pred = random_forest_blender.predict(x_test_predictions)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our accuracy score is 0.9682 which is better than our voting classifier's accuracy of 0.9657.\n\nGreat!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}