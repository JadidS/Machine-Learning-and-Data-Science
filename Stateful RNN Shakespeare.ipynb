{"cells":[{"metadata":{},"cell_type":"markdown","source":"NLP with Stateful RNNs\n\nWe will try to predict Shakepeare Text with an RNN.\n\nContrary to a stateless RNN, a stateful RNN does not throw away the hidden states at every time step and this leads to a model better suited at learning long term patterns."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport os\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install tensorflow==2.0.0-beta1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\nfilepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We download Shakespeare text dataset using keras get_file."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\".join(sorted(set(shakespeare_text.lower())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\ntokenizer.fit_on_texts(shakespeare_text)\nmax_id = len(tokenizer.word_index)\ndataset_size = tokenizer.document_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use kera's tokenizer class to encode every character as an integer.\n\nIt will find all the characters used in the text and map them to a unique character ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\ntrain_size = dataset_size * 90 // 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also split our data into a training set, validation set, and test set.\n\nIn this case we need to avoid any overlap between each set.\n\nWe take 90% of the data for the training set in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = 100\nwindow_length = n_steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the dataset's window method to change this long sequence of characters into multiple smaller windows of text.\n\nSmaller n_steps makes it easier to train with smaller inputs but limits the length of pattern the RNN can learn."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\ndataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\ndataset = dataset.repeat().batch(1)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the windows method uses a nested dataset which cannot be used for training, we must convert it to a tensor using the flat_map to convert it into a flat dataset of tensors.\n\nSince each window is of the same size, we use the batch(window_length) on each window to get a single tensor on each of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nencoded_parts = np.array_split(encoded[:train_size], batch_size)\ndatasets = []\nfor encoded_part in encoded_parts:\n    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n    datasets.append(dataset)\ndataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\ndataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After shuffling the windows, we batch them and seperate the inputs from the last character.\n\nSince categorical input features should generally be encoded, we encode each character using one-hot vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, stateful=True,\n                     dropout=0.2, recurrent_dropout=0.2,\n                     batch_input_shape=[batch_size, None, max_id]),\n    keras.layers.GRU(128, return_sequences=True, stateful=True,\n                     dropout=0.2, recurrent_dropout=0.2),\n    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n                                                    activation=\"softmax\"))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are training a model to predict the next character using the previous 100 characters.\n\nWe use 2 GRU layers with 128 units and 20% dropout on the dataset and the hidden states.\n\nThe output layer is simply a dense layer with 39 units since we find 39 unique characters in the text.\n\nWe compile this model with spare_categorical_crossentropy loos and Adam optimizer.\n\nWe use stateful=True since we want a statefull RNN.\n\nWe also need a batch_input_shape in the first layer since the RNN needs to know the batch size."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResetStatesCallback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to reset states after each epoch before we go to the start of the text using this callback."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(texts): \n    X = np.array(tokenizer.texts_to_sequences(texts)) - 1 \n    return tf.one_hot(X, max_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to preprocess the text before feeding it to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def next_char(text, temperature=1): \n    X_new = preprocess([text]) \n    y_proba = model.predict(X_new)[0, -1:, :] \n    rescaled_logits = tf.math.log(y_proba) / temperature \n    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1 \n    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n\ndef complete_text(text, n_chars=50, temperature=1): \n    for _ in range(n_chars): \n        text += next_char(text, temperature) \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want the next character being guessed to be random. Otherwise the model will predict the same word again and again.\n\nA temparature close to 0 will favor high probability characters while a high temperature will give equal probability to all the characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResetStatesCallback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n        \n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nsteps_per_epoch = train_size // batch_size // n_steps\nmodel.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n                   callbacks=[ResetStatesCallback()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We compile the model and this can take quite some time without a GPU."},{"metadata":{"trusted":true},"cell_type":"code","source":"stateless_model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n                                                    activation=\"softmax\"))\n])\n\nstateless_model.build(tf.TensorShape([None, None, max_id]))\n\nstateless_model.set_weights(model.get_weights())\nmodel = stateless_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(complete_text(\"t\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our output: from most breathe life unto him, with care.\n\nWe finally have some Shakespeare text!\n\nIt could be better but still quite good."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}